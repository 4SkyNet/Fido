\documentclass[letterpaper,12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[nottoc]{tocbibind}
\usepackage[margin=1.25in]{geometry}
\usepackage{mathptmx}
\usepackage[activate={true,nocompatibility},final,
			tracking=true,kerning=true,spacing=true,
			factor=1100,stretch=10,shrink=10]{microtype}
\usepackage{amsmath,setspace} \long\def\/*#1*/{}
\doublespacing

\begin{document}

\/*

The Executive Summary on its own, separate from the Research Report, should convey the essence of your project and should be understood by someone without scientific expertise. Do not simply replicate what you wrote in your Abstract. The summary should clearly present three content areas - the question asked, the methods used and the lessons learned and must be written in layperson (non-specialist) language. NOTE: The summary will be used to explain your project to the general public and in preparing press releases for the media.

*/

\begin{center}
	{\Large
	\textbf{A Universal Robotic Control System using Reinforcement Learning with Limited Feedback}}\\
	\vspace{1cm}
	{\large \textbf{Executive Summary}}
\end{center}

\noindent

A robot intelligence can be simplified into a black box, with inputs such as sensor data and outputs such as motor control.  Most robot software today operates as ``expert systems,'' using preprogrammed conditional logic to execute a set routine.  These implementations are sufficient for the specific purpose and platform they are designed for but lack the ability to perform other tasks or work on other robots.

The purpose of this project was to develop a universal robot control system that can not only adapt to any robot hardware, but can also be trained using positive and negative reinforcement.  The implementation (nicknamed Fido) is lightweight and resource-efficient, making it a practical solution as a control system for mobile robots.  This was achieved by regulating artificial neural networks according to a novel learning algorithm.  The algorithm modifies a well-tested reinforcement learning algorithm called $Q$-learning by fitting a curve to discrete points through a moving least squares interpolator, leading to Fido's high performance.

Fido was tested on a simulated robot with a differential drive system (similar to that of a tank), a large sensor array, and some additional outputs.  The system performed well doing a variety of tasks, such as learning to follow a radio emitter, line following, and drawing basic shapes.  Additionally, results showed that only limited sensor feedback was necessary for learning, further widening possible applications.  Future plans for continuing research include a hardware implementation to test Fido outside of a simulation environment.

\end{document}