A general robotic control system nicknamed Fido was developed that learned tasks with limited feed back. Fido couple the training of artificial neural networks with a wire-fitted moving least squares interpolator to achieve a continuous state-action space $Q$-learning reinforcement algorithm implementation. Fido leveraged a Boltzmann distribution of probability based on reward to select actions, allowing it to continuously explore its state-action space. A kinematically accurate robot was simulated with a differential drive system, a sensor array, and other outputs to test Fido. The robot was trained on a number of common robotic tasks and successfully converged on these tasks in less reward iterations than all other actors tested, while maintaining impressively low latency. In the future, we hope to improve Fido's software further and are working toward the completion of Fido's hardware implementation.